{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "591m7FSBSVV8",
        "outputId": "2a8806cd-f28b-43b3-f1e3-3207f695a518"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Settings\n",
        "filters = 64\n",
        "blocks = 8\n",
        "act = \"relu\"\n",
        "kernel_size = 3\n",
        "\n",
        "# For the JAX Backend\n",
        "# class DepthToSpace(keras.layers.Layer):\n",
        "#     def __init__(self, block_size):\n",
        "#         super().__init__()\n",
        "#         self.block_size = block_size\n",
        "\n",
        "#     def call(self, input):\n",
        "#         batch, height, width, depth = keras.ops.shape(input)\n",
        "#         depth = depth // (self.block_size**2)\n",
        "#         x = keras.ops.reshape(input, [batch, height, width, self.block_size, self.block_size, depth])\n",
        "#         x = keras.ops.transpose(x, [0, 1, 3, 2, 4, 5])\n",
        "#         x = keras.ops.reshape(x, [batch, height * self.block_size, width * self.block_size, depth])\n",
        "#         x = keras.ops.clip(x, 0.0, 1.0)\n",
        "#         return x\n",
        "\n",
        "class DepthToSpace(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = tf.nn.depth_to_space(x, 2)\n",
        "        x = keras.ops.clip(x, 0.0, 1.0)\n",
        "        return x\n",
        "\n",
        "def res_block(input, filters=filters, kernel_size=kernel_size):\n",
        "    x = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same', activation=act)(input)\n",
        "    x = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same', activation=act)(x)\n",
        "    x = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
        "    x = keras.layers.Add()([x, input])\n",
        "    return x\n",
        "\n",
        "# Build the model:\n",
        "inputs = keras.layers.Input(shape=(None,None,1))\n",
        "conv0 = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(inputs)\n",
        "\n",
        "x = conv0\n",
        "for _ in range(blocks):\n",
        "    x = res_block(x)\n",
        "\n",
        "# Feature Fusion\n",
        "conv1 = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
        "mix = keras.layers.Add()([conv1, conv0])\n",
        "\n",
        "# Upsampler\n",
        "features = keras.layers.Conv2D(filters=4, kernel_size=kernel_size, padding='same')(mix)\n",
        "outputs = DepthToSpace()(features)\n",
        "\n",
        "# Defining the model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n",
        "keras.utils.plot_model(model, show_shapes=True)\n",
        "# model.load_weights(\"/content/r8f64.keras\", skip_mismatch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2T_UeEFiD6N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !cp /content/drive/MyDrive/tmp/r8f64.keras /content/r8f64.keras\n",
        "!cp /content/drive/MyDrive/Datasets/Anime_Train_HR.zip /content/HR1.zip\n",
        "!cp /content/drive/MyDrive/Datasets/Digital_Art_Train_HR.zip /content/HR2.zip\n",
        "!unzip /content/HR1.zip\n",
        "!unzip /content/HR2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9fpBPE1OaxR",
        "outputId": "c95dcb3f-1a1c-46ac-c2cf-2f39be697742"
      },
      "outputs": [],
      "source": [
        "# Single Dataset Gray\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "filelist = sorted(glob.glob('/content/HR/*.png'))\n",
        "train_ref = []\n",
        "train_in = []\n",
        "\n",
        "for myFile in filelist:\n",
        "    image = cv2.imread(myFile, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY, 0)\n",
        "    train_ref.append(image)\n",
        "    image = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR_EXACT)\n",
        "    # image = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR_EXACT) # for 4x\n",
        "    train_in.append(image)\n",
        "\n",
        "train_ref = np.array(train_ref).astype(np.float32) / 255.0\n",
        "train_ref = np.clip(train_ref, 0.0, 1.0)\n",
        "train_ref = np.expand_dims(train_ref, axis=-1)\n",
        "print(train_ref.shape)\n",
        "\n",
        "train_in = np.array(train_in).astype(np.float32) / 255.0\n",
        "train_in = np.clip(train_in, 0.0, 1.0)\n",
        "train_in = np.expand_dims(train_in, axis=-1)\n",
        "print(train_in.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9tlaazLNzgQ"
      },
      "outputs": [],
      "source": [
        "# Separate HR and LR\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "filelist1 = sorted(glob.glob('/content/LR/*.png'))\n",
        "train_in = []\n",
        "for myFile in filelist1:\n",
        "    image = cv2.imread(myFile, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY, 0)\n",
        "    # image = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR_EXACT) # for 4x\n",
        "    train_in.append(image)\n",
        "train_in = np.array(train_in).astype(np.float32) / 255.0\n",
        "train_in = np.clip(train_in, 0.0, 1.0)\n",
        "train_in = np.expand_dims(train_in, axis=-1)\n",
        "print(train_in.shape)\n",
        "\n",
        "filelist2 = sorted(glob.glob('/content/HR/*.png'))\n",
        "train_ref = []\n",
        "for myFile in filelist2:\n",
        "    image = cv2.imread(myFile, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY, 0)\n",
        "    train_ref.append(image)\n",
        "train_ref = np.array(train_ref).astype(np.float32) / 255.0\n",
        "train_ref = np.clip(train_ref, 0.0, 1.0)\n",
        "train_ref = np.expand_dims(train_ref, axis=-1)\n",
        "print(train_ref.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_esZ5AQgS3cm",
        "outputId": "c74bc926-90a8-4c40-ccd2-c09d714a48da"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.compile(optimizer=keras.optimizers.AdamW(learning_rate=0.000025), loss=keras.losses.MeanAbsoluteError())\n",
        "history = model.fit(train_in, train_ref, epochs=100, batch_size=16, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnG_vKV3VVuy"
      },
      "outputs": [],
      "source": [
        "# Make a single prediction\n",
        "input = cv2.imread('/content/downscaled.png', cv2.IMREAD_COLOR)\n",
        "input = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY, 0)\n",
        "input = np.array(input).astype(np.float32) / 255.0\n",
        "input = np.clip(input, 0.0, 1.0)\n",
        "input = np.expand_dims(input, axis=0)\n",
        "input = np.expand_dims(input, axis=-1)\n",
        "\n",
        "pred = model.predict(input)\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "pred = np.squeeze(pred)\n",
        "pred = pred * 255.0\n",
        "pred = np.squeeze((np.around(pred)).astype(np.uint8))\n",
        "\n",
        "cv2.imwrite('/content/prediction.png', pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FqgeDySdLqm"
      },
      "outputs": [],
      "source": [
        "# Make a single RGB prediction\n",
        "input = cv2.imread('/content/aoko.png', cv2.IMREAD_COLOR)\n",
        "input = np.array(input).astype(np.float32) / 255.0\n",
        "input = np.clip(input, 0.0, 1.0)\n",
        "(input_b, input_g, input_r) = cv2.split(input)\n",
        "input_b = np.expand_dims(input_b, axis=0)\n",
        "input_g = np.expand_dims(input_g, axis=0)\n",
        "input_r = np.expand_dims(input_r, axis=0)\n",
        "input_b = np.expand_dims(input_b, axis=-1)\n",
        "input_g = np.expand_dims(input_g, axis=-1)\n",
        "input_r = np.expand_dims(input_r, axis=-1)\n",
        "\n",
        "pred_b = model.predict(input_b)\n",
        "pred_g = model.predict(input_g)\n",
        "pred_r = model.predict(input_r)\n",
        "pred = np.stack((pred_b, pred_g, pred_r), axis=-1)\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "pred = np.squeeze(pred)\n",
        "pred = pred * 255.0\n",
        "pred = np.squeeze((np.around(pred)).astype(np.uint8))\n",
        "\n",
        "cv2.imwrite('/content/prediction.png', pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "nKxwNsv5iHBw",
        "outputId": "d4893539-9c40-4e04-e55d-1900cefa057c"
      },
      "outputs": [],
      "source": [
        "!pip install tf2onnx\n",
        "!pip install onnx\n",
        "\n",
        "import tensorflow as tf\n",
        "import tf2onnx\n",
        "import onnx\n",
        "\n",
        "input_signature = [tf.TensorSpec([1, None, None, 1], tf.float32, name='input')]\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model=model, input_signature=input_signature, inputs_as_nchw=['input'], outputs_as_nchw=['depth_to_space'])\n",
        "onnx.save(onnx_model, \"/content/r8f64.onnx\")\n",
        "\n",
        "model.save('/content/r8f64.keras')\n",
        "!cp /content/r8f64.keras /content/drive/MyDrive/tmp/r8f64.keras\n",
        "!cp /content/r8f64.onnx /content/drive/MyDrive/tmp/r8f64.onnx"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
