{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "591m7FSBSVV8",
        "outputId": "2a8806cd-f28b-43b3-f1e3-3207f695a518"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Settings\n",
        "filters = 96\n",
        "blocks = 16\n",
        "act = \"silu\"\n",
        "kernel_size = 3\n",
        "\n",
        "# For the JAX Backend\n",
        "# class DepthToSpace(keras.layers.Layer):\n",
        "#     def __init__(self, block_size):\n",
        "#         super().__init__()\n",
        "#         self.block_size = block_size\n",
        "\n",
        "#     def call(self, input):\n",
        "#         batch, height, width, depth = keras.ops.shape(input)\n",
        "#         depth = depth // (self.block_size**2)\n",
        "#         x = keras.ops.reshape(input, [batch, height, width, self.block_size, self.block_size, depth])\n",
        "#         x = keras.ops.transpose(x, [0, 1, 3, 2, 4, 5])\n",
        "#         x = keras.ops.reshape(x, [batch, height * self.block_size, width * self.block_size, depth])\n",
        "#         if not training:\n",
        "#             x = keras.ops.clip(x, 0.0, 1.0)\n",
        "#         return x\n",
        "\n",
        "class DepthToSpace(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        x = tf.nn.depth_to_space(x, 2)\n",
        "        if not training:\n",
        "            x = keras.ops.clip(x, 0.0, 1.0)\n",
        "        return x\n",
        "\n",
        "def res_block(input, filters=filters, kernel_size=kernel_size):\n",
        "    x = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same', activation=act)(input)\n",
        "    x = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same', activation=act)(x)\n",
        "    x = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
        "    x = keras.layers.Add()([x, input])\n",
        "    return x\n",
        "\n",
        "# Build the model:\n",
        "inputs = keras.layers.Input(shape=(None,None,1))\n",
        "conv0 = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(inputs)\n",
        "\n",
        "x = conv0\n",
        "for _ in range(blocks):\n",
        "    x = res_block(x)\n",
        "\n",
        "# Feature Fusion\n",
        "conv1 = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
        "mix = keras.layers.Add()([conv1, conv0])\n",
        "\n",
        "# Upsampler\n",
        "features = keras.layers.Conv2D(filters=4, kernel_size=kernel_size, padding='same')(mix)\n",
        "outputs = DepthToSpace()(features)\n",
        "\n",
        "# Defining the model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2T_UeEFiD6N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/tmp/r16f96.keras /content/r16f96.keras\n",
        "!cp /content/drive/MyDrive/Datasets/Anime_Train_HR.zip /content/HR1.zip\n",
        "!cp /content/drive/MyDrive/Datasets/Digital_Art_Train_HR.zip /content/HR2.zip\n",
        "!unzip /content/HR1.zip\n",
        "!unzip /content/HR2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Augmentation\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "rotations = [0, 90, 180, 270]\n",
        "\n",
        "filelist = sorted(glob.glob('/content/HR/*.png'))\n",
        "\n",
        "for myFile in tqdm(filelist):\n",
        "    img = cv2.imread(myFile, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    if img is None:\n",
        "        print(f\"Error reading image: {myFile}\")\n",
        "        continue\n",
        "\n",
        "    for rotation in rotations:\n",
        "        rotated_img = np.rot90(img, rotation // 90)\n",
        "        cv2.imwrite(\"/content/HR/\" + str(Path(myFile).stem) + str(rotation) + \".png\", rotated_img)\n",
        "\n",
        "    flipped_img = cv2.flip(img, 1)\n",
        "\n",
        "    for rotation in rotations:\n",
        "        rotated_flipped_img = np.rot90(flipped_img, rotation // 90)\n",
        "        cv2.imwrite(\"/content/HR/\" + str(Path(myFile).stem) + str(rotation) + \"f.png\", rotated_flipped_img)\n",
        "\n",
        "    os.remove(myFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "def data_generator(filelist, batch_size):\n",
        "    n_samples = len(filelist)\n",
        "\n",
        "    while True:\n",
        "        random.shuffle(filelist)\n",
        "\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            batch_files = filelist[i:min(i + batch_size, n_samples)]\n",
        "            train_ref_batch = []\n",
        "            train_in_batch = []\n",
        "\n",
        "            for file in batch_files:\n",
        "                image = cv2.imread(file, cv2.IMREAD_COLOR)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Luma\n",
        "                image = image.astype(np.float32) / 255.0\n",
        "                image = np.clip(image, 0.0, 1.0)\n",
        "\n",
        "                ref_image = np.expand_dims(image.copy(), axis=-1) # Luma\n",
        "                train_ref_batch.append(ref_image)\n",
        "\n",
        "                in_image = cv2.resize(image.copy(), None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR_EXACT) # Box downscale\n",
        "                in_image = np.clip(in_image, 0.0, 1.0)\n",
        "                in_image = np.expand_dims(in_image, axis=-1) # Luma\n",
        "                train_in_batch.append(in_image)\n",
        "\n",
        "            train_ref_batch = np.array(train_ref_batch)\n",
        "            train_in_batch = np.array(train_in_batch)\n",
        "\n",
        "            yield train_in_batch, train_ref_batch\n",
        "\n",
        "\n",
        "filelist = sorted(glob.glob('/content/HR/*.png'))\n",
        "batch_size = 8\n",
        "steps_per_epoch = len(filelist) // batch_size\n",
        "train_generator = data_generator(filelist, batch_size)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.AdamW(learning_rate=0.000025), loss=keras.losses.MeanAbsoluteError(), metrics=[psnr_metric])\n",
        "model.load_weights(\"/content/r16f96.keras\", skip_mismatch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnG_vKV3VVuy"
      },
      "outputs": [],
      "source": [
        "# Make a single prediction\n",
        "input = cv2.imread('/content/downscaled.png', cv2.IMREAD_COLOR)\n",
        "input = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY, 0)\n",
        "input = np.array(input).astype(np.float32) / 255.0\n",
        "input = np.clip(input, 0.0, 1.0)\n",
        "input = np.expand_dims(input, axis=0)\n",
        "input = np.expand_dims(input, axis=-1)\n",
        "\n",
        "pred = model.predict(input)\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "pred = np.squeeze(pred)\n",
        "pred = pred * 255.0\n",
        "pred = np.squeeze((np.around(pred)).astype(np.uint8))\n",
        "\n",
        "cv2.imwrite('/content/prediction.png', pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FqgeDySdLqm"
      },
      "outputs": [],
      "source": [
        "# Make a single RGB prediction\n",
        "input = cv2.imread('/content/aoko.png', cv2.IMREAD_COLOR)\n",
        "input = np.array(input).astype(np.float32) / 255.0\n",
        "input = np.clip(input, 0.0, 1.0)\n",
        "(input_b, input_g, input_r) = cv2.split(input)\n",
        "input_b = np.expand_dims(input_b, axis=0)\n",
        "input_g = np.expand_dims(input_g, axis=0)\n",
        "input_r = np.expand_dims(input_r, axis=0)\n",
        "input_b = np.expand_dims(input_b, axis=-1)\n",
        "input_g = np.expand_dims(input_g, axis=-1)\n",
        "input_r = np.expand_dims(input_r, axis=-1)\n",
        "\n",
        "pred_b = model.predict(input_b)\n",
        "pred_g = model.predict(input_g)\n",
        "pred_r = model.predict(input_r)\n",
        "pred = np.stack((pred_b, pred_g, pred_r), axis=-1)\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "pred = np.squeeze(pred)\n",
        "pred = pred * 255.0\n",
        "pred = np.squeeze((np.around(pred)).astype(np.uint8))\n",
        "\n",
        "cv2.imwrite('/content/prediction.png', pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "nKxwNsv5iHBw",
        "outputId": "d4893539-9c40-4e04-e55d-1900cefa057c"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade onnxsim\n",
        "!pip install tf2onnx\n",
        "!pip install onnx\n",
        "\n",
        "import tensorflow as tf\n",
        "import tf2onnx\n",
        "import onnx\n",
        "\n",
        "input_signature = [tf.TensorSpec([1, None, None, 1], tf.float32, name='input')]\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model=model, input_signature=input_signature, inputs_as_nchw=['input'], outputs_as_nchw=['depth_to_space'])\n",
        "onnx.save(onnx_model, \"/content/r16f96_dev.onnx\")\n",
        "!onnxsim r16f96_dev.onnx r16f96_dev.onnx\n",
        "\n",
        "model.save('/content/r16f96.keras')\n",
        "!cp /content/r16f96.keras /content/drive/MyDrive/tmp/r16f96.keras\n",
        "!cp /content/r16f96.onnx /content/drive/MyDrive/tmp/r16f96.onnx"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
