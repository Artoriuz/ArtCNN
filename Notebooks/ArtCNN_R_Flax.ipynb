{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56VtMIL-GVvi"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade flax\n",
        "!pip install --upgrade orbax-checkpoint\n",
        "!pip install --upgrade jax\n",
        "!pip install --upgrade dm_pix\n",
        "!pip install --upgrade treescope\n",
        "\n",
        "import jax\n",
        "import flax\n",
        "import optax\n",
        "import cv2\n",
        "import glob\n",
        "import treescope\n",
        "import dm_pix as pix\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "\n",
        "# Settings\n",
        "filters = 64\n",
        "blocks = 8\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "class ResBlock(nnx.Module):\n",
        "    def __init__(self, *, rngs: nnx.Rngs):\n",
        "        self.conv0 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.conv1 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.conv2 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, input):\n",
        "        x = nnx.relu(self.conv0(input))\n",
        "        x = nnx.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return x + input\n",
        "\n",
        "class ArtCNN(nnx.Module):\n",
        "    def __init__(self, *, rngs: nnx.Rngs):\n",
        "        self.conv0 = nnx.Conv(1, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.res_blocks = [ResBlock(rngs=rngs) for _ in range(blocks)]\n",
        "        self.conv1 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.feats_conv = nnx.Conv(filters, 4, kernel_size=kernel_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, input):\n",
        "        conv0 = self.conv0(input)\n",
        "        x = conv0\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "        conv1 = self.conv1(x)\n",
        "        features = self.feats_conv(conv1 + conv0)\n",
        "        output = jnp.clip(pix.depth_to_space(features, 2), 0.0, 1.0)\n",
        "        return output\n",
        "\n",
        "model = ArtCNN(rngs=nnx.Rngs(0))\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)\n",
        "nnx.display(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtI6a-2lImmV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/Datasets/Anime_Train_HR.zip /content/HR1.zip\n",
        "!cp /content/drive/MyDrive/Datasets/Digital_Art_Train_HR.zip /content/HR2.zip\n",
        "!unzip /content/HR1.zip\n",
        "!unzip /content/HR2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8V58APqJw71"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/HR\n",
        "!rm -rf /content/LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9fpBPE1OaxR"
      },
      "outputs": [],
      "source": [
        "filelist = sorted(glob.glob('/content/HR/*.png'))\n",
        "train_ref = []\n",
        "train_in = []\n",
        "\n",
        "for myFile in filelist:\n",
        "    image = cv2.imread(myFile, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY, 0)\n",
        "    train_ref.append(image)\n",
        "    image = cv2.resize(image, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR_EXACT)\n",
        "    train_in.append(image)\n",
        "\n",
        "train_ref = np.array(train_ref).astype(np.float32) / 255.0\n",
        "train_ref = np.clip(train_ref, 0.0, 1.0)\n",
        "train_ref = np.expand_dims(train_ref, axis=-1)\n",
        "print(train_ref.shape)\n",
        "train_ref = jnp.array(train_ref)\n",
        "\n",
        "train_in = np.array(train_in).astype(np.float32) / 255.0\n",
        "train_in = np.clip(train_in, 0.0, 1.0)\n",
        "train_in = np.expand_dims(train_in, axis=-1)\n",
        "print(train_in.shape)\n",
        "train_in = jnp.array(train_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAZUjQu4TrIt"
      },
      "outputs": [],
      "source": [
        "optimizer = nnx.Optimizer(model, optax.adamw(0.000025))\n",
        "batch_size = 8\n",
        "num_epochs = 5\n",
        "metrics_history = {'train_loss': []}\n",
        "\n",
        "def create_batches(x, y, batch_size):\n",
        "    num_samples = x.shape[0]\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        x_batch = x[i:i + batch_size]\n",
        "        y_batch = y[i:i + batch_size]\n",
        "        yield x_batch, y_batch\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model, optimizer, x, y):\n",
        "    def loss_fn(model):\n",
        "        y_pred = model(x)\n",
        "        loss = jnp.mean(jnp.absolute(y - y_pred))\n",
        "        return loss\n",
        "\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
        "    optimizer.update(grads)\n",
        "    return loss\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nStarting epoch {epoch + 1} / {num_epochs}\")\n",
        "    epoch_loss = []  # Track loss for this epoch\n",
        "\n",
        "    for step, (x, y) in enumerate(create_batches(train_in, train_ref, batch_size)):\n",
        "        loss = train_step(model, optimizer, x, y)\n",
        "\n",
        "        metrics_history['train_loss'].append(loss)\n",
        "        epoch_loss.append(loss)\n",
        "\n",
        "        if step % 50 == 0:  # Print every 50 steps\n",
        "            print(f\"[train] epoch: {epoch + 1}, step: {step}, batch loss: {loss:.6f}\")\n",
        "\n",
        "    # Print epoch summary\n",
        "    epoch_avg_loss = jnp.mean(jnp.array(epoch_loss))\n",
        "    print(f\"Epoch {epoch + 1} complete. Average loss: {epoch_avg_loss:.6f}\")\n",
        "    save_checkpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snjQFwdV7oqg"
      },
      "outputs": [],
      "source": [
        "from flax import nnx\n",
        "import orbax.checkpoint as ocp\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "def save_checkpoint():\n",
        "    ckpt_dir = ocp.test_utils.erase_and_create_empty('/content/checkpoints/')\n",
        "\n",
        "    _, state = nnx.split(model)\n",
        "    print(\"Creating Checkpoint\")\n",
        "    checkpointer = ocp.StandardCheckpointer()\n",
        "    checkpointer.save(ckpt_dir / 'state', state)\n",
        "    print(\"Sending Checkpoint to Google Drive\")\n",
        "    !zip -r /content/checkpoints.zip /content/checkpoints/*\n",
        "    !cp /content/checkpoints.zip /content/drive/MyDrive/tmp/checkpoints.zip\n",
        "\n",
        "save_checkpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5C95v9RPj3K"
      },
      "outputs": [],
      "source": [
        "from flax import nnx\n",
        "import orbax.checkpoint as ocp\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "\n",
        "abstract_model = nnx.eval_shape(lambda: ArtCNN(rngs=nnx.Rngs(0)))\n",
        "graphdef, abstract_state = nnx.split(abstract_model)\n",
        "print('The abstract NNX state (all leaves are abstract arrays):')\n",
        "nnx.display(abstract_state)\n",
        "\n",
        "state_restored = checkpointer.restore('/content/checkpoints/state', abstract_state)\n",
        "print('NNX State restored: ')\n",
        "nnx.display(state_restored)\n",
        "\n",
        "model = nnx.merge(graphdef, state_restored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4DalaEl894d"
      },
      "outputs": [],
      "source": [
        "# Make a single prediction\n",
        "input = cv2.imread('/content/downscaled.png', cv2.IMREAD_COLOR)\n",
        "input = cv2.cvtColor(input, cv2.COLOR_BGR2GRAY, 0)\n",
        "input = np.array(input).astype(np.float32) / 255.0\n",
        "input = np.clip(input, 0.0, 1.0)\n",
        "input = np.expand_dims(input, axis=0)\n",
        "input = np.expand_dims(input, axis=-1)\n",
        "\n",
        "pred = model(jnp.array(input))\n",
        "pred = np.clip(np.array(pred), 0.0, 1.0)\n",
        "pred = np.squeeze(pred)\n",
        "pred = pred * 255.0\n",
        "pred = np.squeeze((np.around(pred)).astype(np.uint8))\n",
        "\n",
        "cv2.imwrite('/content/prediction_jax.png', pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HyNYFBp93MN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tf2onnx\n",
        "!pip install --upgrade onnx\n",
        "!pip install --upgrade onnxsim\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import tf2onnx\n",
        "import onnx\n",
        "import tensorflow as tf\n",
        "from jax.experimental import jax2tf\n",
        "\n",
        "# Convert JAX function to TensorFlow function\n",
        "tf_function = tf.function(\n",
        "    jax2tf.convert(model, polymorphic_shapes=[\"(1, h, w, 1)\"], enable_xla=False),\n",
        "    input_signature=[tf.TensorSpec([1, None, None, 1], dtype=tf.float32, name=\"input\")]\n",
        ")\n",
        "\n",
        "# Export TensorFlow function to ONNX\n",
        "onnx_model_path = \"ArtCNN_R8F64_Flax.onnx\"\n",
        "\n",
        "# Note: input_signature must match the input signature used for tf.function()\n",
        "onnx_model, _ = tf2onnx.convert.from_function(\n",
        "    tf_function,\n",
        "    input_signature=[tf.TensorSpec([1, None, None, 1], dtype=tf.float32, name=\"input\")],\n",
        "    output_path=\"ArtCNN_R8F64_Flax.onnx\",  # File path for the ONNX model\n",
        "    inputs_as_nchw=['input'],  # Specify input tensor name as 'input' (ensure NCHW format)\n",
        "    outputs_as_nchw=['output'],  # Specify output tensor name as 'depth_to_space' (NCHW format)\n",
        "    opset=17  # Use opset 13 to avoid issues with unsupported operations\n",
        ")\n",
        "\n",
        "!onnxsim ArtCNN_R8F64_Flax.onnx ArtCNN_R8F64_Flax.onnx"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
