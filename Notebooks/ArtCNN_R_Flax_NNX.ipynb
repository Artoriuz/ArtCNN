{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "56VtMIL-GVvi"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import flax\n",
        "import optax\n",
        "import cv2\n",
        "import glob\n",
        "import treescope\n",
        "import dm_pix as pix\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import orbax.checkpoint as ocp\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from flax import nnx\n",
        "\n",
        "# Settings\n",
        "filters = 64\n",
        "blocks = 8\n",
        "kernel_size = (3, 3)\n",
        "\n",
        "class DepthToSpace(nnx.Module):\n",
        "    def __init__(self, *, rngs: nnx.Rngs):\n",
        "        self.block_size = 2\n",
        "\n",
        "    def __call__(self, input):\n",
        "        x = pix.depth_to_space(input, self.block_size)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nnx.Module):\n",
        "    def __init__(self, *, rngs: nnx.Rngs):\n",
        "        self.conv0 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.conv1 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.conv2 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "\n",
        "    def __call__(self, input):\n",
        "        x = nnx.silu(self.conv0(input))\n",
        "        x = nnx.silu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return x + input\n",
        "\n",
        "class ArtCNN(nnx.Module):\n",
        "    def __init__(self, *, rngs: nnx.Rngs):\n",
        "        self.conv0 = nnx.Conv(1, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.res_blocks = [ResBlock(rngs=rngs) for _ in range(blocks)]\n",
        "        self.conv1 = nnx.Conv(filters, filters, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.feats_conv = nnx.Conv(filters, 4, kernel_size=kernel_size, rngs=rngs)\n",
        "        self.depth_to_space = DepthToSpace(rngs=rngs)\n",
        "\n",
        "    def __call__(self, input):\n",
        "        conv0 = self.conv0(input)\n",
        "        x = conv0\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "        conv1 = self.conv1(x)\n",
        "        x = conv1 + conv0\n",
        "        x = self.feats_conv(x)\n",
        "        x = self.depth_to_space(x)\n",
        "        x = jnp.clip(x, 0.0, 1.0)\n",
        "        return x\n",
        "\n",
        "model = ArtCNN(rngs=nnx.Rngs(0))\n",
        "treescope.basic_interactive_setup(autovisualize_arrays=True)\n",
        "nnx.display(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAnO6UVfFZV2"
      },
      "outputs": [],
      "source": [
        "abstract_model = nnx.eval_shape(lambda: ArtCNN(rngs=nnx.Rngs(0)))\n",
        "graphdef, abstract_state = nnx.split(abstract_model)\n",
        "\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "state_restored = checkpointer.restore('/content/checkpoints/state', abstract_state)\n",
        "nnx.display(state_restored)\n",
        "\n",
        "model = nnx.merge(graphdef, state_restored)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtI6a-2lImmV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/Datasets/Anime_Train_HR.zip /content/HR1.zip\n",
        "!cp /content/drive/MyDrive/Datasets/Digital_Art_Train_HR.zip /content/HR2.zip\n",
        "!unzip /content/HR1.zip\n",
        "!unzip /content/HR2.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8V58APqJw71"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/HR\n",
        "!rm -rf /content/LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f9fpBPE1OaxR"
      },
      "outputs": [],
      "source": [
        "def data_generator(filelist, batch_size):\n",
        "    n_samples = len(filelist)\n",
        "\n",
        "    while True:\n",
        "        random.shuffle(filelist)\n",
        "\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            batch_files = filelist[i:min(i + batch_size, n_samples)]\n",
        "            train_ref_batch = []\n",
        "            train_in_batch = []\n",
        "\n",
        "            for file in batch_files:\n",
        "                image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
        "                image = image.astype(np.float32) / 255.0\n",
        "                image = np.clip(image, 0.0, 1.0)\n",
        "\n",
        "                ref_image = np.expand_dims(image.copy(), axis=-1) # Luma\n",
        "                train_ref_batch.append(ref_image)\n",
        "\n",
        "                in_image = cv2.resize(image.copy(), None, fx=0.5, fy=0.5, interpolation=cv2.INTER_LINEAR_EXACT) # Box downscale\n",
        "                in_image = np.clip(in_image, 0.0, 1.0)\n",
        "                in_image = np.expand_dims(in_image, axis=-1) # Luma\n",
        "                train_in_batch.append(in_image)\n",
        "\n",
        "            train_ref_batch = np.array(train_ref_batch)\n",
        "            train_in_batch = np.array(train_in_batch)\n",
        "\n",
        "            yield train_in_batch, train_ref_batch\n",
        "\n",
        "def loss_fn(pred, target):\n",
        "    return jnp.mean(jnp.absolute(target - pred))\n",
        "\n",
        "def forward(model, input, target):\n",
        "    pred = model(input)\n",
        "    loss = loss_fn(pred, target)\n",
        "    return loss\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model, optimizer, input, target):\n",
        "    loss, grads = nnx.value_and_grad(forward)(model, input, target)\n",
        "    optimizer.update(model, grads)\n",
        "    return loss\n",
        "\n",
        "learning_rate = 0.0001\n",
        "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate), wrt=nnx.Param)\n",
        "epochs = 5\n",
        "batch_size = 8\n",
        "filelist = sorted(glob.glob('/content/HR/*.png'))\n",
        "steps_per_epoch = len(filelist) // batch_size\n",
        "train_generator = data_generator(filelist, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    with tqdm(total=steps_per_epoch, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"step\") as pbar:\n",
        "        for step in range(steps_per_epoch):\n",
        "            batch_in, batch_ref = next(train_generator)\n",
        "            loss = train_step(model, optimizer, batch_in, batch_ref)\n",
        "            epoch_loss += float(loss)\n",
        "\n",
        "            pbar.set_postfix(loss=float(loss))\n",
        "            pbar.update(1)\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / steps_per_epoch\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}: Average Loss = {avg_epoch_loss:.6f}, Time = {elapsed:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snjQFwdV7oqg"
      },
      "outputs": [],
      "source": [
        "_, state = nnx.split(model)\n",
        "nnx.display(state)\n",
        "\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "checkpointer.save('/content/checkpoints/state', state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i28GwqvZini"
      },
      "outputs": [],
      "source": [
        "# Make a single prediction\n",
        "input = cv2.imread('/content/downscaled.png', cv2.IMREAD_GRAYSCALE)\n",
        "input = np.array(input).astype(np.float32) / 255.0\n",
        "input = np.clip(input, 0.0, 1.0)\n",
        "input = np.expand_dims(input, axis=0)\n",
        "input = np.expand_dims(input, axis=-1)\n",
        "input = jnp.array(input)\n",
        "\n",
        "pred = model(input)\n",
        "pred = np.array(pred)\n",
        "pred = np.clip(pred, 0.0, 1.0)\n",
        "pred = np.squeeze(pred)\n",
        "pred = pred * 255.0\n",
        "pred = np.squeeze((np.around(pred)).astype(np.uint8))\n",
        "\n",
        "cv2.imwrite('/content/prediction.png', pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
